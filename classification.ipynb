{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.io \n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' \n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl_origin = torch.load('Dataset/train_dl.pt')\n",
    "valid_dl_origin = torch.load('Dataset/valid_dl.pt')\n",
    "\n",
    "train_CSI = train_dl_origin.dataset[:][0]\n",
    "train_label = train_dl_origin.dataset[:][1][:,2].type(torch.LongTensor)\n",
    "\n",
    "valid_CSI = valid_dl_origin.dataset[:][0]\n",
    "valid_label = valid_dl_origin.dataset[:][1][:,2].type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate modulus and angle values, then combine into 1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a_raw = torch.angle(train_CSI).squeeze()\n",
    "train_m_raw = torch.abs(train_CSI).squeeze()\n",
    "valid_a_raw = torch.angle(valid_CSI).squeeze()\n",
    "valid_m_raw = torch.abs(valid_CSI).squeeze()\n",
    "train_am_raw = torch.stack((train_a_raw, train_m_raw), dim=3)\n",
    "valid_am_raw = torch.stack((valid_a_raw, valid_m_raw), dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15000, 4, 1632])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_a_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape to matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_am_raw = train_am_raw.reshape((train_am_raw.shape[0], 4*1632*2))\n",
    "valid_am_raw = valid_am_raw.reshape((valid_am_raw.shape[0], 4*1632*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize to mean 0 std 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.mean(train_am_raw, dim=0)\n",
    "std = torch.std(train_am_raw, dim=0)\n",
    "train_am = (train_am_raw-mean)/(std)\n",
    "valid_am = (valid_am_raw-mean)/(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression, accuracy of 100% on training, 90% on valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-26 {color: black;background-color: white;}#sk-container-id-26 pre{padding: 0;}#sk-container-id-26 div.sk-toggleable {background-color: white;}#sk-container-id-26 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-26 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-26 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-26 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-26 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-26 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-26 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-26 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-26 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-26 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-26 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-26 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-26 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-26 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-26 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-26 div.sk-item {position: relative;z-index: 1;}#sk-container-id-26 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-26 div.sk-item::before, #sk-container-id-26 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-26 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-26 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-26 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-26 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-26 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-26 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-26 div.sk-label-container {text-align: center;}#sk-container-id-26 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-26 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-26\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" checked><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=10000)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(max_iter=10000)\n",
    "lr.fit(train_am, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.8954)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(train_label, lr.predict(train_am)), accuracy_score(valid_label, lr.predict(valid_am))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM, accuracy of __ % on training, __ % on valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(train_am, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9862, 0.9472)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(train_label, svc.predict(train_am)), accuracy_score(valid_label, svc.predict(valid_am))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did not try KNN due to high dimensionality of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest using modulus and angle is much better, accuracy of 100% on training, 99.06% on valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(train_am, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.9906)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(train_label, rf.predict(train_am)), accuracy_score(valid_label, rf.predict(valid_am))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network to classify LoS and NLoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (fcs): ModuleList(\n",
      "    (0): Linear(in_features=13056, out_features=1000, bias=True)\n",
      "    (1): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.fcs = nn.ModuleList([nn.Linear(input_size, 1000), nn.Linear(1000, 100)])\n",
    "        self.output = nn.Linear(100, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()  \n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for fc in self.fcs:\n",
    "            x = self.relu(self.dropout(fc(x)))\n",
    "        x = self.output(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x.squeeze()\n",
    "        \n",
    "        \n",
    "input_size = train_am.shape[1]\n",
    "model = Model(input_size)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Training Loss: 0.4373367726802826, Test Loss: 0.44296348094940186\n",
      "Epoch [2/200], Training Loss: 0.4260183870792389, Test Loss: 0.4327827990055084\n",
      "Epoch [3/200], Training Loss: 0.4199770390987396, Test Loss: 0.4278047978878021\n",
      "Epoch [4/200], Training Loss: 0.39976730942726135, Test Loss: 0.4104239344596863\n",
      "Epoch [5/200], Training Loss: 0.38852372765541077, Test Loss: 0.401488721370697\n",
      "Epoch [6/200], Training Loss: 0.3922261595726013, Test Loss: 0.4059831202030182\n",
      "Epoch [7/200], Training Loss: 0.3840775787830353, Test Loss: 0.40156984329223633\n",
      "Epoch [8/200], Training Loss: 0.3579406440258026, Test Loss: 0.37753522396087646\n",
      "Epoch [9/200], Training Loss: 0.3286665976047516, Test Loss: 0.3558156490325928\n",
      "Epoch [10/200], Training Loss: 0.37912267446517944, Test Loss: 0.39937520027160645\n",
      "Epoch [11/200], Training Loss: 0.28174102306365967, Test Loss: 0.3193577229976654\n",
      "Epoch [12/200], Training Loss: 0.3038058280944824, Test Loss: 0.33667659759521484\n",
      "Epoch [13/200], Training Loss: 0.23316732048988342, Test Loss: 0.27819210290908813\n",
      "Epoch [14/200], Training Loss: 0.21206875145435333, Test Loss: 0.262696236371994\n",
      "Epoch [15/200], Training Loss: 0.355339378118515, Test Loss: 0.3948131203651428\n",
      "Epoch [16/200], Training Loss: 0.18966959416866302, Test Loss: 0.2422693818807602\n",
      "Epoch [17/200], Training Loss: 0.1605827510356903, Test Loss: 0.22124315798282623\n",
      "Epoch [18/200], Training Loss: 0.14376796782016754, Test Loss: 0.20512516796588898\n",
      "Epoch [19/200], Training Loss: 0.12820468842983246, Test Loss: 0.1954655647277832\n",
      "Epoch [20/200], Training Loss: 0.11619073897600174, Test Loss: 0.18483881652355194\n",
      "Epoch [21/200], Training Loss: 0.10415378957986832, Test Loss: 0.1733541190624237\n",
      "Epoch [22/200], Training Loss: 0.09779635071754456, Test Loss: 0.1716776192188263\n",
      "Epoch [23/200], Training Loss: 0.13311009109020233, Test Loss: 0.23186670243740082\n",
      "Epoch [24/200], Training Loss: 0.09481525421142578, Test Loss: 0.16118782758712769\n",
      "Epoch [25/200], Training Loss: 0.07657957077026367, Test Loss: 0.14585435390472412\n",
      "Epoch [26/200], Training Loss: 0.06733927130699158, Test Loss: 0.1437048465013504\n",
      "Epoch [27/200], Training Loss: 0.06604859232902527, Test Loss: 0.13498052954673767\n",
      "Epoch [28/200], Training Loss: 0.06102879345417023, Test Loss: 0.13250450789928436\n",
      "Epoch [29/200], Training Loss: 0.05387471988797188, Test Loss: 0.13768205046653748\n",
      "Epoch [30/200], Training Loss: 0.05055669695138931, Test Loss: 0.12130362540483475\n",
      "Epoch [31/200], Training Loss: 0.04550033435225487, Test Loss: 0.12585404515266418\n",
      "Epoch [32/200], Training Loss: 0.043610673397779465, Test Loss: 0.11626314371824265\n",
      "Epoch [33/200], Training Loss: 0.04501008614897728, Test Loss: 0.11544183641672134\n",
      "Epoch [34/200], Training Loss: 0.03684023767709732, Test Loss: 0.12215256690979004\n",
      "Epoch [35/200], Training Loss: 0.038530297577381134, Test Loss: 0.10833065211772919\n",
      "Epoch [36/200], Training Loss: 0.031512532383203506, Test Loss: 0.10371683537960052\n",
      "Epoch [37/200], Training Loss: 0.03125712648034096, Test Loss: 0.10122115164995193\n",
      "Epoch [38/200], Training Loss: 0.029905524104833603, Test Loss: 0.10600332915782928\n",
      "Epoch [39/200], Training Loss: 0.03553996980190277, Test Loss: 0.10382939130067825\n",
      "Epoch [40/200], Training Loss: 0.02444775030016899, Test Loss: 0.09791169315576553\n",
      "Epoch [41/200], Training Loss: 0.023767374455928802, Test Loss: 0.09726203978061676\n",
      "Epoch [42/200], Training Loss: 0.03217608109116554, Test Loss: 0.12889014184474945\n",
      "Epoch [43/200], Training Loss: 0.02565867081284523, Test Loss: 0.0937349796295166\n",
      "Epoch [44/200], Training Loss: 0.021257711574435234, Test Loss: 0.09203067421913147\n",
      "Epoch [45/200], Training Loss: 0.020918408408761024, Test Loss: 0.09191494435071945\n",
      "Epoch [46/200], Training Loss: 0.020517054945230484, Test Loss: 0.0917893573641777\n",
      "Epoch [47/200], Training Loss: 0.019976196810603142, Test Loss: 0.10262206941843033\n",
      "Epoch [48/200], Training Loss: 0.019007019698619843, Test Loss: 0.09220406413078308\n",
      "Epoch [49/200], Training Loss: 0.018041936680674553, Test Loss: 0.08761654049158096\n",
      "Epoch [50/200], Training Loss: 0.01686275191605091, Test Loss: 0.08771724998950958\n",
      "Epoch [51/200], Training Loss: 0.016176141798496246, Test Loss: 0.08662436157464981\n",
      "Epoch [52/200], Training Loss: 0.016836050897836685, Test Loss: 0.08751403540372849\n",
      "Epoch [53/200], Training Loss: 0.015284466557204723, Test Loss: 0.08540308475494385\n",
      "Epoch [54/200], Training Loss: 0.015099408105015755, Test Loss: 0.0830027386546135\n",
      "Epoch [55/200], Training Loss: 0.014730485156178474, Test Loss: 0.0822930708527565\n",
      "Epoch [56/200], Training Loss: 0.014842151664197445, Test Loss: 0.08173219114542007\n",
      "Epoch [57/200], Training Loss: 0.013439157977700233, Test Loss: 0.08970578759908676\n",
      "Epoch [58/200], Training Loss: 0.0129869244992733, Test Loss: 0.08608207106590271\n",
      "Epoch [59/200], Training Loss: 0.013124333694577217, Test Loss: 0.07935399562120438\n",
      "Epoch [60/200], Training Loss: 0.013307789340615273, Test Loss: 0.08234847337007523\n",
      "Epoch [61/200], Training Loss: 0.013706912286579609, Test Loss: 0.07864358276128769\n",
      "Epoch [62/200], Training Loss: 0.011593758128583431, Test Loss: 0.0807686522603035\n",
      "Epoch [63/200], Training Loss: 0.013576604425907135, Test Loss: 0.07854428142309189\n",
      "Epoch [64/200], Training Loss: 0.010967475362122059, Test Loss: 0.08136389404535294\n",
      "Epoch [65/200], Training Loss: 0.01190578006207943, Test Loss: 0.07785309851169586\n",
      "Epoch [66/200], Training Loss: 0.012062717229127884, Test Loss: 0.07616034150123596\n",
      "Epoch [67/200], Training Loss: 0.010869834572076797, Test Loss: 0.07627955079078674\n",
      "Epoch [68/200], Training Loss: 0.009545588865876198, Test Loss: 0.07678166031837463\n",
      "Epoch [69/200], Training Loss: 0.010800211690366268, Test Loss: 0.07670900970697403\n",
      "Epoch [70/200], Training Loss: 0.009690853767096996, Test Loss: 0.08073368668556213\n",
      "Epoch [71/200], Training Loss: 0.009274541400372982, Test Loss: 0.078186996281147\n",
      "Epoch [72/200], Training Loss: 0.008900932036340237, Test Loss: 0.076397605240345\n",
      "Epoch [73/200], Training Loss: 0.009127388708293438, Test Loss: 0.07413223385810852\n",
      "Epoch [74/200], Training Loss: 0.008241470903158188, Test Loss: 0.07983841001987457\n",
      "Epoch [75/200], Training Loss: 0.00820398423820734, Test Loss: 0.07863204181194305\n",
      "Epoch [76/200], Training Loss: 0.008334564045071602, Test Loss: 0.07766719907522202\n",
      "Epoch [77/200], Training Loss: 0.007878893986344337, Test Loss: 0.0791943371295929\n",
      "Epoch [78/200], Training Loss: 0.008120287209749222, Test Loss: 0.08253640681505203\n",
      "Epoch [79/200], Training Loss: 0.007557423785328865, Test Loss: 0.08054661005735397\n",
      "Epoch [80/200], Training Loss: 0.00879448838531971, Test Loss: 0.07380300015211105\n",
      "Epoch [81/200], Training Loss: 0.0075856600888073444, Test Loss: 0.07352705299854279\n",
      "Epoch [82/200], Training Loss: 0.007316443603485823, Test Loss: 0.07354423403739929\n",
      "Epoch [83/200], Training Loss: 0.008177255280315876, Test Loss: 0.0725596472620964\n",
      "Epoch [84/200], Training Loss: 0.007357348687946796, Test Loss: 0.08049221336841583\n",
      "Epoch [85/200], Training Loss: 0.007123046554625034, Test Loss: 0.07962502539157867\n",
      "Epoch [86/200], Training Loss: 0.00664548622444272, Test Loss: 0.07369296252727509\n",
      "Epoch [87/200], Training Loss: 0.006892184726893902, Test Loss: 0.07271122932434082\n",
      "Epoch [88/200], Training Loss: 0.006667199078947306, Test Loss: 0.07613535225391388\n",
      "Epoch [89/200], Training Loss: 0.006477648392319679, Test Loss: 0.07170971482992172\n",
      "Epoch [90/200], Training Loss: 0.007045155391097069, Test Loss: 0.07381284981966019\n",
      "Epoch [91/200], Training Loss: 0.006678332574665546, Test Loss: 0.07160072028636932\n",
      "Epoch [92/200], Training Loss: 0.006333399564027786, Test Loss: 0.07683838158845901\n",
      "Epoch [93/200], Training Loss: 0.006058555096387863, Test Loss: 0.07210469990968704\n",
      "Epoch [94/200], Training Loss: 0.005727929528802633, Test Loss: 0.07252175360918045\n",
      "Epoch [95/200], Training Loss: 0.006292126141488552, Test Loss: 0.07029274851083755\n",
      "Epoch [96/200], Training Loss: 0.005843665450811386, Test Loss: 0.07052699476480484\n",
      "Epoch [97/200], Training Loss: 0.005475214682519436, Test Loss: 0.07162556797266006\n",
      "Epoch [98/200], Training Loss: 0.005359500180929899, Test Loss: 0.07686766982078552\n",
      "Epoch [99/200], Training Loss: 0.005764010827988386, Test Loss: 0.07517535239458084\n",
      "Epoch [100/200], Training Loss: 0.0059723928570747375, Test Loss: 0.07333362102508545\n",
      "Epoch [101/200], Training Loss: 0.0064532458782196045, Test Loss: 0.07013892382383347\n",
      "Epoch [102/200], Training Loss: 0.0057097845710814, Test Loss: 0.06982892751693726\n",
      "Epoch [103/200], Training Loss: 0.005210684146732092, Test Loss: 0.07130283117294312\n",
      "Epoch [104/200], Training Loss: 0.005700743291527033, Test Loss: 0.07097601890563965\n",
      "Epoch [105/200], Training Loss: 0.005324867088347673, Test Loss: 0.07289119064807892\n",
      "Epoch [106/200], Training Loss: 0.005088124889880419, Test Loss: 0.07376876473426819\n",
      "Epoch [107/200], Training Loss: 0.005312614142894745, Test Loss: 0.07015880197286606\n",
      "Epoch [108/200], Training Loss: 0.005128907039761543, Test Loss: 0.07707702368497849\n",
      "Epoch [109/200], Training Loss: 0.004958530887961388, Test Loss: 0.07556122541427612\n",
      "Epoch [110/200], Training Loss: 0.005725949537009001, Test Loss: 0.06950373202562332\n",
      "Epoch [111/200], Training Loss: 0.005044456105679274, Test Loss: 0.07207504659891129\n",
      "Epoch [112/200], Training Loss: 0.005272633861750364, Test Loss: 0.0786687359213829\n",
      "Epoch [113/200], Training Loss: 0.004982059355825186, Test Loss: 0.07116258144378662\n",
      "Epoch [114/200], Training Loss: 0.00502806156873703, Test Loss: 0.06813854724168777\n",
      "Epoch [115/200], Training Loss: 0.004989915993064642, Test Loss: 0.06823050230741501\n",
      "Epoch [116/200], Training Loss: 0.004714231472462416, Test Loss: 0.0677763819694519\n",
      "Epoch [117/200], Training Loss: 0.004539568908512592, Test Loss: 0.07012033462524414\n",
      "Epoch [118/200], Training Loss: 0.004971812944859266, Test Loss: 0.06869654357433319\n",
      "Epoch [119/200], Training Loss: 0.005072350613772869, Test Loss: 0.06889359652996063\n",
      "Epoch [120/200], Training Loss: 0.01329448726028204, Test Loss: 0.0924399271607399\n",
      "Epoch [121/200], Training Loss: 0.004919379483908415, Test Loss: 0.07060368359088898\n",
      "Epoch [122/200], Training Loss: 0.0048289429396390915, Test Loss: 0.06679300963878632\n",
      "Epoch [123/200], Training Loss: 0.004360502120107412, Test Loss: 0.0683230310678482\n",
      "Epoch [124/200], Training Loss: 0.004503780975937843, Test Loss: 0.06630267202854156\n",
      "Epoch [125/200], Training Loss: 0.004479140043258667, Test Loss: 0.06601627171039581\n",
      "Epoch [126/200], Training Loss: 0.004460620693862438, Test Loss: 0.0701838880777359\n",
      "Epoch [127/200], Training Loss: 0.004263656213879585, Test Loss: 0.06839846819639206\n",
      "Epoch [128/200], Training Loss: 0.004730396904051304, Test Loss: 0.06599382311105728\n",
      "Epoch [129/200], Training Loss: 0.004171851556748152, Test Loss: 0.06731124222278595\n",
      "Epoch [130/200], Training Loss: 0.004143789876252413, Test Loss: 0.06795039027929306\n",
      "Epoch [131/200], Training Loss: 0.004023256711661816, Test Loss: 0.067747101187706\n",
      "Epoch [132/200], Training Loss: 0.004072978626936674, Test Loss: 0.06919220834970474\n",
      "Epoch [133/200], Training Loss: 0.004380649421364069, Test Loss: 0.06656252592802048\n",
      "Epoch [134/200], Training Loss: 0.004060340579599142, Test Loss: 0.06802625954151154\n",
      "Epoch [135/200], Training Loss: 0.004034978803247213, Test Loss: 0.06853442639112473\n",
      "Epoch [136/200], Training Loss: 0.0040823183953762054, Test Loss: 0.07115853577852249\n",
      "Epoch [137/200], Training Loss: 0.004547345917671919, Test Loss: 0.06627240031957626\n",
      "Epoch [138/200], Training Loss: 0.003917552065104246, Test Loss: 0.06672489643096924\n",
      "Epoch [139/200], Training Loss: 0.003844302147626877, Test Loss: 0.06617303937673569\n",
      "Epoch [140/200], Training Loss: 0.0044379220344126225, Test Loss: 0.06509651988744736\n",
      "Epoch [141/200], Training Loss: 0.004124115686863661, Test Loss: 0.06978409737348557\n",
      "Epoch [142/200], Training Loss: 0.004132296424359083, Test Loss: 0.06533364206552505\n",
      "Epoch [143/200], Training Loss: 0.0037648293655365705, Test Loss: 0.06641413271427155\n",
      "Epoch [144/200], Training Loss: 0.0038068233989179134, Test Loss: 0.06974108517169952\n",
      "Epoch [145/200], Training Loss: 0.003699741791933775, Test Loss: 0.06777358055114746\n",
      "Epoch [146/200], Training Loss: 0.0038137389346957207, Test Loss: 0.06562186032533646\n",
      "Epoch [147/200], Training Loss: 0.0036926986649632454, Test Loss: 0.06542723625898361\n",
      "Epoch [148/200], Training Loss: 0.003463084576651454, Test Loss: 0.06629738211631775\n",
      "Epoch [149/200], Training Loss: 0.0033869051840156317, Test Loss: 0.06999010592699051\n",
      "Epoch [150/200], Training Loss: 0.0039850762113928795, Test Loss: 0.065644770860672\n",
      "Epoch [151/200], Training Loss: 0.003272739704698324, Test Loss: 0.06723586469888687\n",
      "Epoch [152/200], Training Loss: 0.0031203916296362877, Test Loss: 0.066842220723629\n",
      "Epoch [153/200], Training Loss: 0.0031299758702516556, Test Loss: 0.06556067615747452\n",
      "Epoch [154/200], Training Loss: 0.0029894511681050062, Test Loss: 0.0668037086725235\n",
      "Epoch [155/200], Training Loss: 0.0028839034494012594, Test Loss: 0.07072927802801132\n",
      "Epoch [156/200], Training Loss: 0.0029910386074334383, Test Loss: 0.07265476137399673\n",
      "Epoch [157/200], Training Loss: 0.002917116740718484, Test Loss: 0.07122858613729477\n",
      "Epoch [158/200], Training Loss: 0.0030380759853869677, Test Loss: 0.06891486793756485\n",
      "Epoch [159/200], Training Loss: 0.0030595988500863314, Test Loss: 0.06684818863868713\n",
      "Epoch [160/200], Training Loss: 0.0028856713324785233, Test Loss: 0.06776397675275803\n",
      "Epoch [161/200], Training Loss: 0.0029059946537017822, Test Loss: 0.067336305975914\n",
      "Epoch [162/200], Training Loss: 0.003104503033682704, Test Loss: 0.06535033136606216\n",
      "Epoch [163/200], Training Loss: 0.003101793583482504, Test Loss: 0.06833197921514511\n",
      "Epoch [164/200], Training Loss: 0.003084092168137431, Test Loss: 0.06702745705842972\n",
      "Epoch [165/200], Training Loss: 0.0029487800784409046, Test Loss: 0.06912840902805328\n",
      "Epoch [166/200], Training Loss: 0.003104978473857045, Test Loss: 0.06534169614315033\n",
      "Epoch [167/200], Training Loss: 0.0029785740189254284, Test Loss: 0.06761165708303452\n",
      "Epoch [168/200], Training Loss: 0.0028440472669899464, Test Loss: 0.06746037304401398\n",
      "Epoch [169/200], Training Loss: 0.0029445935506373644, Test Loss: 0.06761372089385986\n",
      "Epoch [170/200], Training Loss: 0.002920265542343259, Test Loss: 0.07186766713857651\n",
      "Epoch [171/200], Training Loss: 0.0029275272972881794, Test Loss: 0.06708985567092896\n",
      "Epoch [172/200], Training Loss: 0.0028365820180624723, Test Loss: 0.06566763669252396\n",
      "Epoch [173/200], Training Loss: 0.0027295260224491358, Test Loss: 0.06645242869853973\n",
      "Epoch [174/200], Training Loss: 0.002830809447914362, Test Loss: 0.06707419455051422\n",
      "Epoch [175/200], Training Loss: 0.002859413158148527, Test Loss: 0.06467752903699875\n",
      "Epoch [176/200], Training Loss: 0.0026515237987041473, Test Loss: 0.06833384186029434\n",
      "Epoch [177/200], Training Loss: 0.0028569793794304132, Test Loss: 0.07237327098846436\n",
      "Epoch [178/200], Training Loss: 0.0027486211620271206, Test Loss: 0.06990569084882736\n",
      "Epoch [179/200], Training Loss: 0.003260513534769416, Test Loss: 0.06442809104919434\n",
      "Epoch [180/200], Training Loss: 0.0026529617607593536, Test Loss: 0.06737708300352097\n",
      "Epoch [181/200], Training Loss: 0.0025479879695922136, Test Loss: 0.0670325830578804\n",
      "Epoch [182/200], Training Loss: 0.002577677136287093, Test Loss: 0.06999211013317108\n",
      "Epoch [183/200], Training Loss: 0.0026789484545588493, Test Loss: 0.06521306931972504\n",
      "Epoch [184/200], Training Loss: 0.002697258023545146, Test Loss: 0.06459057331085205\n",
      "Epoch [185/200], Training Loss: 0.0025882229674607515, Test Loss: 0.06586285680532455\n",
      "Epoch [186/200], Training Loss: 0.0029303492046892643, Test Loss: 0.06485776603221893\n",
      "Epoch [187/200], Training Loss: 0.0026603054720908403, Test Loss: 0.06637264043092728\n",
      "Epoch [188/200], Training Loss: 0.002603447064757347, Test Loss: 0.065390944480896\n",
      "Epoch [189/200], Training Loss: 0.0025826634373515844, Test Loss: 0.06444089114665985\n",
      "Epoch [190/200], Training Loss: 0.002452092943713069, Test Loss: 0.06717051565647125\n",
      "Epoch [191/200], Training Loss: 0.00264517730101943, Test Loss: 0.06698136031627655\n",
      "Epoch [192/200], Training Loss: 0.002657210687175393, Test Loss: 0.06691507250070572\n",
      "Epoch [193/200], Training Loss: 0.002659651217982173, Test Loss: 0.06535506993532181\n",
      "Epoch [194/200], Training Loss: 0.0026263974141329527, Test Loss: 0.0654854103922844\n",
      "Epoch [195/200], Training Loss: 0.002544092247262597, Test Loss: 0.06371979415416718\n",
      "Epoch [196/200], Training Loss: 0.002266045194119215, Test Loss: 0.06735038757324219\n",
      "Epoch [197/200], Training Loss: 0.0023347116075456142, Test Loss: 0.0662376806139946\n",
      "Epoch [198/200], Training Loss: 0.0027006082236766815, Test Loss: 0.0630071684718132\n",
      "Epoch [199/200], Training Loss: 0.0023100057151168585, Test Loss: 0.07044312357902527\n",
      "Epoch [200/200], Training Loss: 0.0022847491782158613, Test Loss: 0.06520023941993713\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(train_am, train_label.to(torch.float32))\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "best_loss = 0.06520023941993713 # best loss achieved from previous training\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Check the validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(valid_am)\n",
    "        test_loss = loss_fn(outputs, valid_label.to(torch.float32))\n",
    "        # do training loss as well due to dropout biasing the loss when in training mode\n",
    "        outputs = model(train_am)\n",
    "        loss = loss_fn(outputs, train_label.to(torch.float32))\n",
    "    if (test_loss.item() < best_loss):\n",
    "        best_loss = test_loss.item()\n",
    "        torch.save(model.state_dict(), 'best-model-parameters-los.pt')\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {loss.item()}, Test Loss: {test_loss.item()}')\n",
    "\n",
    "print('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.98)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "train_out = model(train_am)\n",
    "train_out = [1 if i > .5 else 0 for i in train_out]\n",
    "outputs = model(valid_am)\n",
    "outputs = [1 if i > .5 else 0 for i in outputs]\n",
    "accuracy_score(train_label, train_out), accuracy_score(valid_label, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model, 1 hidden layer, dropout(.5), ReLU and sigmoid, BCE Loss, lr=0.01\n",
    "\n",
    "Training accuracy 100%, validation accuracy 98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.98)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = Model(input_size)\n",
    "best.load_state_dict(torch.load(\"best-model-parameters-los.pt\"))\n",
    "best.eval()\n",
    "train_out = model(train_am)\n",
    "train_out = [1 if i > .5 else 0 for i in train_out]\n",
    "outputs = model(valid_am)\n",
    "test_loss = loss_fn(outputs, valid_label.to(torch.float32))\n",
    "outputs = [1 if i > .5 else 0 for i in outputs]\n",
    "accuracy_score(train_label, train_out), accuracy_score(valid_label, outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
