{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.io \n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' \n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl_origin = torch.load('Dataset/train_dl.pt')\n",
    "valid_dl_origin = torch.load('Dataset/valid_dl.pt')\n",
    "\n",
    "train_CSI = train_dl_origin.dataset[:][0]\n",
    "train_label = train_dl_origin.dataset[:][1][:,0:2]\n",
    "train_x_label = train_label[:,0]\n",
    "train_y_label = train_label[:,1]\n",
    "\n",
    "valid_CSI = valid_dl_origin.dataset[:][0]\n",
    "valid_label = valid_dl_origin.dataset[:][1][:,0:2]\n",
    "valid_x_label = valid_label[:,0]\n",
    "valid_y_label = valid_label[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate modulus and angle values, then combine into 1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a_raw = torch.angle(train_CSI).squeeze()\n",
    "train_m_raw = torch.abs(train_CSI).squeeze()\n",
    "valid_a_raw = torch.angle(valid_CSI).squeeze()\n",
    "valid_m_raw = torch.abs(valid_CSI).squeeze()\n",
    "train_am_raw = torch.stack((train_a_raw, train_m_raw), dim=3)\n",
    "valid_am_raw = torch.stack((valid_a_raw, valid_m_raw), dim=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape to matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_am_raw = train_am_raw.reshape((train_am_raw.shape[0], 4*1632*2))\n",
    "valid_am_raw = valid_am_raw.reshape((valid_am_raw.shape[0], 4*1632*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize to mean 0 std 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.mean(train_am_raw, dim=0)\n",
    "std = torch.std(train_am_raw, dim=0)\n",
    "train_am = (train_am_raw-mean)/(std)\n",
    "valid_am = (valid_am_raw-mean)/(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr_x = LinearRegression()\n",
    "lr_x.fit(train_am, train_x_label)\n",
    "lr_y = LinearRegression()\n",
    "lr_y.fit(train_am, train_y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M.A.P.E train x:  0.28542122\n",
      "M.A.P.E train y:  0.2976161\n",
      "M.S.E. train x:  2.6430402\n",
      "M.S.E. train y:  1.0316823\n"
     ]
    }
   ],
   "source": [
    "pred_train_x = lr_x.predict(train_am)\n",
    "pred_train_y = lr_y.predict(train_am)\n",
    "print(\"M.A.P.E train x: \", mean_absolute_percentage_error(train_x_label, pred_train_x))\n",
    "print(\"M.A.P.E train y: \", mean_absolute_percentage_error(train_y_label, pred_train_y))\n",
    "print(\"M.S.E. train x: \", mean_squared_error(train_x_label, pred_train_x))\n",
    "print(\"M.S.E. train y: \", mean_squared_error(train_y_label, pred_train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M.A.P.E valid x:  2.556207\n",
      "M.A.P.E valid y:  3.145411\n",
      "M.S.E. valid x:  167.06215\n",
      "M.S.E. valid y:  68.51464\n"
     ]
    }
   ],
   "source": [
    "pred_valid_x = lr_x.predict(valid_am)\n",
    "pred_valid_y = lr_y.predict(valid_am)\n",
    "print(\"M.A.P.E valid x: \", mean_absolute_percentage_error(valid_x_label, pred_valid_x))\n",
    "print(\"M.A.P.E valid y: \", mean_absolute_percentage_error(valid_y_label, pred_valid_y))\n",
    "print(\"M.S.E. valid x: \", mean_squared_error(valid_x_label, pred_valid_x))\n",
    "print(\"M.S.E. valid y: \", mean_squared_error(valid_y_label, pred_valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVR()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr_x = SVR()\n",
    "svr_x.fit(train_am, train_x_label)\n",
    "svr_y = SVR()\n",
    "svr_y.fit(train_am, train_y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_v/2js8j3rj2bx2wbdqwf9wzghw0000gn/T/ipykernel_20786/2592972992.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpred_train_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvr_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_am\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpred_train_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvr_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_am\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"M.A.P.E train x: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_percentage_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_train_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"M.A.P.E train y: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_percentage_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"M.S.E. train x: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_train_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_dense_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0msvm_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLIBSVM_IMPL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         return libsvm.predict(\n\u001b[0m\u001b[1;32m    455\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pred_train_x = svr_x.predict(train_am)\n",
    "pred_train_y = svr_y.predict(train_am)\n",
    "print(\"M.A.P.E train x: \", mean_absolute_percentage_error(train_x_label, pred_train_x))\n",
    "print(\"M.A.P.E train y: \", mean_absolute_percentage_error(train_y_label, pred_train_y))\n",
    "print(\"M.S.E. train x: \", mean_squared_error(train_x_label, pred_train_x))\n",
    "print(\"M.S.E. train y: \", mean_squared_error(train_y_label, pred_train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid_x = svr_x.predict(valid_am)\n",
    "pred_valid_y = svr_y.predict(valid_am)\n",
    "print(\"M.A.P.E valid x: \", mean_absolute_percentage_error(valid_x_label, pred_valid_x))\n",
    "print(\"M.A.P.E valid y: \", mean_absolute_percentage_error(valid_y_label, pred_valid_y))\n",
    "print(\"M.S.E. valid x: \", mean_squared_error(valid_x_label, pred_valid_x))\n",
    "print(\"M.S.E. valid y: \", mean_squared_error(valid_y_label, pred_valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=10)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(max_depth=10)  # when max_depth is not set, it overfits, with depth=29\n",
    "dt.fit(train_am, train_x_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = [10, 15, 20, 25, 30]\n",
    "min_splits = [2, 4, 6, 8, 10]\n",
    "results = [] #(tree itself, (depth, split), (train % err, valid % err))\n",
    "for max_depth in max_depths:\n",
    "    for min_split in min_splits:\n",
    "        dt = DecisionTreeRegressor(max_depth=max_depth, min_samples_split=min_split)\n",
    "        dt.fit(train_am, train_x_label)\n",
    "        pred_train_x = dt.predict(train_am)\n",
    "        train_err_x = mean_absolute_percentage_error(train_x_label, pred_train_x)\n",
    "        pred_valid_x = dt.predict(valid_am)\n",
    "        valid_err_x = mean_absolute_percentage_error(valid_x_label, pred_valid_x)\n",
    "        results.append((dt, (max_depth, min_splits), (train_err_x, valid_err_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.006232623054380457, 0.4579651457707525)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min = 1000\n",
    "min_train = 1000\n",
    "dt_x = None\n",
    "for result in results:\n",
    "    if result[2][1] < min:\n",
    "        min = result[2][1]\n",
    "        min_train = result[2][0]\n",
    "        dt_x = result[0]\n",
    "min_train, min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min_samples_split: 4 \n",
      "Max depth 25\n"
     ]
    }
   ],
   "source": [
    "print(\"Min_samples_split:\",dt_x.min_samples_split,\"\\nMax depth\", dt_x.max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train it on the y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=25, min_samples_split=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=25, min_samples_split=4)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=25, min_samples_split=4)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_y = DecisionTreeRegressor(max_depth = dt_x.max_depth, min_samples_split=dt_x.min_samples_split)\n",
    "dt_y.fit(train_am, train_y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M.A.P.E train x:  0.006232623054380457\n",
      "M.A.P.E train y:  0.006992053435375219\n",
      "M.S.E. train x:  0.08776657553184125\n",
      "M.S.E. train y:  0.03687887105764928\n"
     ]
    }
   ],
   "source": [
    "pred_train_x = dt_x.predict(train_am)\n",
    "pred_train_y = dt_y.predict(train_am)\n",
    "print(\"M.A.P.E train x: \", mean_absolute_percentage_error(train_x_label, pred_train_x))\n",
    "print(\"M.A.P.E train y: \", mean_absolute_percentage_error(train_y_label, pred_train_y))\n",
    "print(\"M.S.E. train x: \", mean_squared_error(train_x_label, pred_train_x))\n",
    "print(\"M.S.E. train y: \", mean_squared_error(train_y_label, pred_train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M.A.P.E valid x:  0.4579651457707525\n",
      "M.A.P.E valid y:  0.45093538697782604\n",
      "M.S.E. valid x:  22.204276844652814\n",
      "M.S.E. valid y:  9.963308777646168\n"
     ]
    }
   ],
   "source": [
    "pred_valid_x = dt_x.predict(valid_am)\n",
    "pred_valid_y = dt_y.predict(valid_am)\n",
    "print(\"M.A.P.E valid x: \", mean_absolute_percentage_error(valid_x_label, pred_valid_x))\n",
    "print(\"M.A.P.E valid y: \", mean_absolute_percentage_error(valid_y_label, pred_valid_y))\n",
    "print(\"M.S.E. valid x: \", mean_squared_error(valid_x_label, pred_valid_x))\n",
    "print(\"M.S.E. valid y: \", mean_squared_error(valid_y_label, pred_valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (fcs): ModuleList(\n",
      "    (0): Linear(in_features=13056, out_features=3000, bias=True)\n",
      "    (1): Linear(in_features=3000, out_features=1000, bias=True)\n",
      "    (2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (3): Linear(in_features=100, out_features=20, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=20, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (relu): ELU(alpha=1.0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.fcs = nn.ModuleList([nn.Linear(input_size, 3000), \n",
    "                                  nn.Linear(3000,1000), \n",
    "                                  nn.Linear(1000, 100),\n",
    "                                  nn.Linear(100, 20)])\n",
    "        self.output = nn.Linear(20, 2)\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "        self.elu = nn.ELU()  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        for fc in self.fcs:\n",
    "            x = self.elu(self.dropout(fc(x)))\n",
    "        x = self.output(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "input_size = train_am.shape[1]\n",
    "model = Model(input_size)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Training Loss: 133.4093780517578, Test Loss: 60.88287353515625\n",
      "Epoch [2/200], Training Loss: 90.5941162109375, Test Loss: 54.6991081237793\n",
      "Epoch [3/200], Training Loss: 76.77713775634766, Test Loss: 40.30664825439453\n",
      "Epoch [4/200], Training Loss: 72.37869262695312, Test Loss: 34.32225799560547\n",
      "Epoch [5/200], Training Loss: 66.85469055175781, Test Loss: 36.408424377441406\n",
      "Epoch [6/200], Training Loss: 62.7216796875, Test Loss: 42.368221282958984\n",
      "Epoch [7/200], Training Loss: 57.934818267822266, Test Loss: 21.906322479248047\n",
      "Epoch [8/200], Training Loss: 56.797508239746094, Test Loss: 24.221954345703125\n",
      "Epoch [9/200], Training Loss: 52.85347366333008, Test Loss: 20.355812072753906\n",
      "Epoch [10/200], Training Loss: 52.529762268066406, Test Loss: 18.508981704711914\n",
      "Epoch [11/200], Training Loss: 51.120758056640625, Test Loss: 13.629170417785645\n",
      "Epoch [12/200], Training Loss: 47.83685302734375, Test Loss: 13.20617389678955\n",
      "Epoch [13/200], Training Loss: 49.1103515625, Test Loss: 13.000970840454102\n",
      "Epoch [14/200], Training Loss: 45.60942077636719, Test Loss: 9.61344051361084\n",
      "Epoch [15/200], Training Loss: 45.48357009887695, Test Loss: 13.936864852905273\n",
      "Epoch [16/200], Training Loss: 45.978599548339844, Test Loss: 8.753311157226562\n",
      "Epoch [17/200], Training Loss: 45.6290168762207, Test Loss: 13.044510841369629\n",
      "Epoch [18/200], Training Loss: 46.19492721557617, Test Loss: 7.79539680480957\n",
      "Epoch [19/200], Training Loss: 46.14280319213867, Test Loss: 7.655062675476074\n",
      "Epoch [20/200], Training Loss: 44.32310104370117, Test Loss: 8.285407066345215\n",
      "Epoch [21/200], Training Loss: 43.42136764526367, Test Loss: 8.318595886230469\n",
      "Epoch [22/200], Training Loss: 42.104713439941406, Test Loss: 12.243521690368652\n",
      "Epoch [23/200], Training Loss: 43.20088195800781, Test Loss: 11.353052139282227\n",
      "Epoch [24/200], Training Loss: 43.97382354736328, Test Loss: 8.700478553771973\n",
      "Epoch [25/200], Training Loss: 44.089778900146484, Test Loss: 7.212560176849365\n",
      "Epoch [26/200], Training Loss: 44.100101470947266, Test Loss: 7.771170616149902\n",
      "Epoch [27/200], Training Loss: 42.65646743774414, Test Loss: 6.972290992736816\n",
      "Epoch [28/200], Training Loss: 42.831478118896484, Test Loss: 6.417587757110596\n",
      "Epoch [29/200], Training Loss: 42.02647018432617, Test Loss: 6.713426113128662\n",
      "Epoch [30/200], Training Loss: 42.940879821777344, Test Loss: 6.672770977020264\n",
      "Epoch [31/200], Training Loss: 44.574302673339844, Test Loss: 6.409448623657227\n",
      "Epoch [32/200], Training Loss: 42.70948028564453, Test Loss: 6.542844295501709\n",
      "Epoch [33/200], Training Loss: 43.920631408691406, Test Loss: 7.011686325073242\n",
      "Epoch [34/200], Training Loss: 42.534732818603516, Test Loss: 6.623560428619385\n",
      "Epoch [35/200], Training Loss: 43.810176849365234, Test Loss: 8.720977783203125\n",
      "Epoch [36/200], Training Loss: 43.234195709228516, Test Loss: 5.804558753967285\n",
      "Epoch [37/200], Training Loss: 43.181331634521484, Test Loss: 5.710885047912598\n",
      "Epoch [38/200], Training Loss: 42.510005950927734, Test Loss: 5.440560817718506\n",
      "Epoch [39/200], Training Loss: 43.93571472167969, Test Loss: 6.969165802001953\n",
      "Epoch [40/200], Training Loss: 44.573387145996094, Test Loss: 5.9900922775268555\n",
      "Epoch [41/200], Training Loss: 41.90272903442383, Test Loss: 5.447064399719238\n",
      "Epoch [42/200], Training Loss: 43.47360610961914, Test Loss: 5.698789119720459\n",
      "Epoch [43/200], Training Loss: 42.473899841308594, Test Loss: 7.924221992492676\n",
      "Epoch [44/200], Training Loss: 40.43642044067383, Test Loss: 5.5750603675842285\n",
      "Epoch [45/200], Training Loss: 41.65546417236328, Test Loss: 8.740958213806152\n",
      "Epoch [46/200], Training Loss: 42.59922409057617, Test Loss: 5.230656623840332\n",
      "Epoch [47/200], Training Loss: 43.42978286743164, Test Loss: 5.548777103424072\n",
      "Epoch [48/200], Training Loss: 41.85546875, Test Loss: 5.618050575256348\n",
      "Epoch [49/200], Training Loss: 42.53109359741211, Test Loss: 4.79608678817749\n",
      "Epoch [50/200], Training Loss: 41.969966888427734, Test Loss: 4.665129661560059\n",
      "Epoch [51/200], Training Loss: 42.48740005493164, Test Loss: 5.813313007354736\n",
      "Epoch [52/200], Training Loss: 41.10729217529297, Test Loss: 5.870604515075684\n",
      "Epoch [53/200], Training Loss: 42.40997314453125, Test Loss: 6.493406772613525\n",
      "Epoch [54/200], Training Loss: 42.797576904296875, Test Loss: 4.98936128616333\n",
      "Epoch [55/200], Training Loss: 42.45465850830078, Test Loss: 6.003085136413574\n",
      "Epoch [56/200], Training Loss: 42.38814163208008, Test Loss: 5.170478343963623\n",
      "Epoch [57/200], Training Loss: 42.25432205200195, Test Loss: 7.175378799438477\n",
      "Epoch [58/200], Training Loss: 41.752784729003906, Test Loss: 5.0280914306640625\n",
      "Epoch [59/200], Training Loss: 40.9405403137207, Test Loss: 5.529333114624023\n",
      "Epoch [60/200], Training Loss: 41.41319274902344, Test Loss: 4.735495090484619\n",
      "Epoch [61/200], Training Loss: 43.220523834228516, Test Loss: 5.40675687789917\n",
      "Epoch [62/200], Training Loss: 41.35963821411133, Test Loss: 5.932169437408447\n",
      "Epoch [63/200], Training Loss: 43.2371711730957, Test Loss: 4.751404762268066\n",
      "Epoch [64/200], Training Loss: 41.10664367675781, Test Loss: 5.358276844024658\n",
      "Epoch [65/200], Training Loss: 43.22367477416992, Test Loss: 5.86205530166626\n",
      "Epoch [66/200], Training Loss: 43.2205696105957, Test Loss: 5.366044044494629\n",
      "Epoch [67/200], Training Loss: 42.16558837890625, Test Loss: 4.958281993865967\n",
      "Epoch [68/200], Training Loss: 41.5250244140625, Test Loss: 4.850939750671387\n",
      "Epoch [69/200], Training Loss: 40.16111755371094, Test Loss: 5.187891483306885\n",
      "Epoch [70/200], Training Loss: 40.31283950805664, Test Loss: 5.933066368103027\n",
      "Epoch [71/200], Training Loss: 41.92869567871094, Test Loss: 5.179385662078857\n",
      "Epoch [72/200], Training Loss: 42.32088088989258, Test Loss: 5.406652450561523\n",
      "Epoch [73/200], Training Loss: 40.421180725097656, Test Loss: 4.525157451629639\n",
      "Epoch [74/200], Training Loss: 42.23467254638672, Test Loss: 5.223572731018066\n",
      "Epoch [75/200], Training Loss: 40.4038200378418, Test Loss: 4.9253010749816895\n",
      "Epoch [76/200], Training Loss: 43.22731399536133, Test Loss: 4.419811248779297\n",
      "Epoch [77/200], Training Loss: 41.013328552246094, Test Loss: 5.8993635177612305\n",
      "Epoch [78/200], Training Loss: 41.26008605957031, Test Loss: 4.697276592254639\n",
      "Epoch [79/200], Training Loss: 40.91307830810547, Test Loss: 4.86325216293335\n",
      "Epoch [80/200], Training Loss: 41.9935417175293, Test Loss: 4.868747234344482\n",
      "Epoch [81/200], Training Loss: 41.27927780151367, Test Loss: 5.5765228271484375\n",
      "Epoch [82/200], Training Loss: 38.912269592285156, Test Loss: 6.663584232330322\n",
      "Epoch [83/200], Training Loss: 41.834716796875, Test Loss: 5.088210582733154\n",
      "Epoch [84/200], Training Loss: 41.284732818603516, Test Loss: 5.025670051574707\n",
      "Epoch [85/200], Training Loss: 42.014076232910156, Test Loss: 5.145317554473877\n",
      "Epoch [86/200], Training Loss: 41.949554443359375, Test Loss: 5.224658489227295\n",
      "Epoch [87/200], Training Loss: 42.8023567199707, Test Loss: 5.556215286254883\n",
      "Epoch [88/200], Training Loss: 40.7314453125, Test Loss: 6.222709655761719\n",
      "Epoch [89/200], Training Loss: 42.09805679321289, Test Loss: 5.853786945343018\n",
      "Epoch [90/200], Training Loss: 40.2972297668457, Test Loss: 7.330738544464111\n",
      "Epoch [91/200], Training Loss: 40.57004165649414, Test Loss: 4.871249675750732\n",
      "Epoch [92/200], Training Loss: 42.47503662109375, Test Loss: 5.0269269943237305\n",
      "Epoch [93/200], Training Loss: 41.41752243041992, Test Loss: 4.564009666442871\n",
      "Epoch [94/200], Training Loss: 42.95197296142578, Test Loss: 4.832706451416016\n",
      "Epoch [95/200], Training Loss: 41.484127044677734, Test Loss: 4.708388805389404\n",
      "Epoch [96/200], Training Loss: 42.09720993041992, Test Loss: 5.724126815795898\n",
      "Epoch [97/200], Training Loss: 41.167972564697266, Test Loss: 5.161348819732666\n",
      "Epoch [98/200], Training Loss: 41.60374450683594, Test Loss: 5.380622863769531\n",
      "Epoch [99/200], Training Loss: 40.736324310302734, Test Loss: 4.957942962646484\n",
      "Epoch [100/200], Training Loss: 39.57992935180664, Test Loss: 4.836732864379883\n",
      "Epoch [101/200], Training Loss: 41.46185302734375, Test Loss: 5.914168357849121\n",
      "Epoch [102/200], Training Loss: 41.88227081298828, Test Loss: 4.63326358795166\n",
      "Epoch [103/200], Training Loss: 40.26035690307617, Test Loss: 5.268710136413574\n",
      "Epoch [104/200], Training Loss: 41.784305572509766, Test Loss: 5.163860321044922\n",
      "Epoch [105/200], Training Loss: 39.9947624206543, Test Loss: 5.257690906524658\n",
      "Epoch [106/200], Training Loss: 42.371337890625, Test Loss: 4.840095043182373\n",
      "Epoch [107/200], Training Loss: 41.42878341674805, Test Loss: 4.425820350646973\n",
      "Epoch [108/200], Training Loss: 40.68453598022461, Test Loss: 4.54878044128418\n",
      "Epoch [109/200], Training Loss: 40.79807662963867, Test Loss: 5.438035488128662\n",
      "Epoch [110/200], Training Loss: 40.84275817871094, Test Loss: 4.976282596588135\n",
      "Epoch [111/200], Training Loss: 39.94266891479492, Test Loss: 5.018136024475098\n",
      "Epoch [112/200], Training Loss: 40.168113708496094, Test Loss: 6.1023101806640625\n",
      "Epoch [113/200], Training Loss: 40.11954879760742, Test Loss: 5.721029758453369\n",
      "Epoch [114/200], Training Loss: 40.33662414550781, Test Loss: 4.867562294006348\n",
      "Epoch [115/200], Training Loss: 40.86625289916992, Test Loss: 6.842808246612549\n",
      "Epoch [116/200], Training Loss: 40.10112380981445, Test Loss: 4.895811080932617\n",
      "Epoch [117/200], Training Loss: 43.55131149291992, Test Loss: 4.806825637817383\n",
      "Epoch [118/200], Training Loss: 42.221805572509766, Test Loss: 4.795421600341797\n",
      "Epoch [119/200], Training Loss: 40.173500061035156, Test Loss: 5.49271297454834\n",
      "Epoch [120/200], Training Loss: 39.92250061035156, Test Loss: 5.527741432189941\n",
      "Epoch [121/200], Training Loss: 41.18584060668945, Test Loss: 5.314513683319092\n",
      "Epoch [122/200], Training Loss: 38.76846694946289, Test Loss: 5.310783863067627\n",
      "Epoch [123/200], Training Loss: 38.064491271972656, Test Loss: 5.88378381729126\n",
      "Epoch [124/200], Training Loss: 41.13050079345703, Test Loss: 6.214273452758789\n",
      "Epoch [125/200], Training Loss: 40.75776672363281, Test Loss: 4.66644287109375\n",
      "Epoch [126/200], Training Loss: 42.11141586303711, Test Loss: 4.401579856872559\n",
      "Epoch [127/200], Training Loss: 39.99526596069336, Test Loss: 5.270794868469238\n",
      "Epoch [128/200], Training Loss: 41.211647033691406, Test Loss: 4.958003997802734\n",
      "Epoch [129/200], Training Loss: 41.652156829833984, Test Loss: 5.177016258239746\n",
      "Epoch [130/200], Training Loss: 39.900211334228516, Test Loss: 5.01276159286499\n",
      "Epoch [131/200], Training Loss: 41.70702362060547, Test Loss: 5.524223804473877\n",
      "Epoch [132/200], Training Loss: 43.662254333496094, Test Loss: 5.103851795196533\n",
      "Epoch [133/200], Training Loss: 41.875526428222656, Test Loss: 6.146075248718262\n",
      "Epoch [134/200], Training Loss: 39.44213104248047, Test Loss: 5.282556056976318\n",
      "Epoch [135/200], Training Loss: 43.445777893066406, Test Loss: 5.119781494140625\n",
      "Epoch [136/200], Training Loss: 40.532958984375, Test Loss: 7.155208587646484\n",
      "Epoch [137/200], Training Loss: 40.85089111328125, Test Loss: 5.757842063903809\n",
      "Epoch [138/200], Training Loss: 39.953819274902344, Test Loss: 5.424162864685059\n",
      "Epoch [139/200], Training Loss: 43.68064498901367, Test Loss: 4.831964015960693\n",
      "Epoch [140/200], Training Loss: 39.619361877441406, Test Loss: 6.089663028717041\n",
      "Epoch [141/200], Training Loss: 41.723785400390625, Test Loss: 5.287968635559082\n",
      "Epoch [142/200], Training Loss: 41.34864807128906, Test Loss: 4.843714237213135\n",
      "Epoch [143/200], Training Loss: 38.15672302246094, Test Loss: 5.644969463348389\n",
      "Epoch [144/200], Training Loss: 40.91230773925781, Test Loss: 4.810720920562744\n",
      "Epoch [145/200], Training Loss: 41.36796951293945, Test Loss: 5.130769729614258\n",
      "Epoch [146/200], Training Loss: 40.0821533203125, Test Loss: 5.251368045806885\n",
      "Epoch [147/200], Training Loss: 40.45692825317383, Test Loss: 6.064659118652344\n",
      "Epoch [148/200], Training Loss: 40.345550537109375, Test Loss: 5.317311763763428\n",
      "Epoch [149/200], Training Loss: 41.8759651184082, Test Loss: 5.589132308959961\n",
      "Epoch [150/200], Training Loss: 40.67420196533203, Test Loss: 5.66469669342041\n",
      "Epoch [151/200], Training Loss: 40.6800651550293, Test Loss: 5.47393798828125\n",
      "Epoch [152/200], Training Loss: 40.45029830932617, Test Loss: 6.158743381500244\n",
      "Epoch [153/200], Training Loss: 42.19947052001953, Test Loss: 5.003301620483398\n",
      "Epoch [154/200], Training Loss: 40.091365814208984, Test Loss: 5.918686866760254\n",
      "Epoch [155/200], Training Loss: 40.06430435180664, Test Loss: 4.548379421234131\n",
      "Epoch [156/200], Training Loss: 40.79549789428711, Test Loss: 5.206505298614502\n",
      "Epoch [157/200], Training Loss: 40.89186096191406, Test Loss: 4.561500072479248\n",
      "Epoch [158/200], Training Loss: 41.391170501708984, Test Loss: 5.083706855773926\n",
      "Epoch [159/200], Training Loss: 40.497581481933594, Test Loss: 4.540173530578613\n",
      "Epoch [160/200], Training Loss: 42.44239044189453, Test Loss: 4.785454750061035\n",
      "Epoch [161/200], Training Loss: 40.39611053466797, Test Loss: 4.3815598487854\n",
      "Epoch [162/200], Training Loss: 41.29146194458008, Test Loss: 6.279181003570557\n",
      "Epoch [163/200], Training Loss: 41.363460540771484, Test Loss: 5.10460090637207\n",
      "Epoch [164/200], Training Loss: 40.67984390258789, Test Loss: 5.011314868927002\n",
      "Epoch [165/200], Training Loss: 41.41855239868164, Test Loss: 4.623082160949707\n",
      "Epoch [166/200], Training Loss: 40.045738220214844, Test Loss: 4.713212490081787\n",
      "Epoch [167/200], Training Loss: 42.96892166137695, Test Loss: 4.905882835388184\n",
      "Epoch [168/200], Training Loss: 40.48947525024414, Test Loss: 5.296789169311523\n",
      "Epoch [169/200], Training Loss: 39.78902053833008, Test Loss: 8.526700973510742\n",
      "Epoch [170/200], Training Loss: 42.17881774902344, Test Loss: 5.15212345123291\n",
      "Epoch [171/200], Training Loss: 40.541656494140625, Test Loss: 6.327340602874756\n",
      "Epoch [172/200], Training Loss: 40.58697509765625, Test Loss: 4.401710033416748\n",
      "Epoch [173/200], Training Loss: 39.1859130859375, Test Loss: 4.884891033172607\n",
      "Epoch [174/200], Training Loss: 39.92916488647461, Test Loss: 4.676171779632568\n",
      "Epoch [175/200], Training Loss: 42.08197784423828, Test Loss: 5.558493614196777\n",
      "Epoch [176/200], Training Loss: 38.9283561706543, Test Loss: 5.731657981872559\n",
      "Epoch [177/200], Training Loss: 39.622764587402344, Test Loss: 4.705063819885254\n",
      "Epoch [178/200], Training Loss: 41.73735046386719, Test Loss: 4.914265155792236\n",
      "Epoch [179/200], Training Loss: 40.190895080566406, Test Loss: 4.529428958892822\n",
      "Epoch [180/200], Training Loss: 38.662498474121094, Test Loss: 5.679239273071289\n",
      "Epoch [181/200], Training Loss: 39.6751708984375, Test Loss: 5.8040900230407715\n",
      "Epoch [182/200], Training Loss: 40.38640213012695, Test Loss: 5.5573272705078125\n",
      "Epoch [183/200], Training Loss: 40.46488571166992, Test Loss: 5.08298397064209\n",
      "Epoch [184/200], Training Loss: 40.99354934692383, Test Loss: 5.128023624420166\n",
      "Epoch [185/200], Training Loss: 40.7155876159668, Test Loss: 5.456644058227539\n",
      "Epoch [186/200], Training Loss: 40.00696563720703, Test Loss: 5.732349872589111\n",
      "Epoch [187/200], Training Loss: 41.89480972290039, Test Loss: 4.782691478729248\n",
      "Epoch [188/200], Training Loss: 42.37681198120117, Test Loss: 6.366743087768555\n",
      "Epoch [189/200], Training Loss: 39.41975021362305, Test Loss: 5.039021968841553\n",
      "Epoch [190/200], Training Loss: 40.980628967285156, Test Loss: 5.809175968170166\n",
      "Epoch [191/200], Training Loss: 41.50638198852539, Test Loss: 5.69632625579834\n",
      "Epoch [192/200], Training Loss: 41.27500534057617, Test Loss: 4.830097675323486\n",
      "Epoch [193/200], Training Loss: 41.15427780151367, Test Loss: 4.64111852645874\n",
      "Epoch [194/200], Training Loss: 40.92558288574219, Test Loss: 6.107127666473389\n",
      "Epoch [195/200], Training Loss: 40.914817810058594, Test Loss: 5.570292949676514\n",
      "Epoch [196/200], Training Loss: 41.44809341430664, Test Loss: 5.7540059089660645\n",
      "Epoch [197/200], Training Loss: 38.88447952270508, Test Loss: 5.639763355255127\n",
      "Epoch [198/200], Training Loss: 40.14956283569336, Test Loss: 5.144436836242676\n",
      "Epoch [199/200], Training Loss: 38.847103118896484, Test Loss: 5.717418670654297\n",
      "Epoch [200/200], Training Loss: 40.68922805786133, Test Loss: 5.376333236694336\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "model = Model(input_size)\n",
    "dataset = TensorDataset(train_am, train_label.to(torch.float32))\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "val_dataset = TensorDataset(valid_am, valid_label.to(torch.float32))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True) \n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "#tried adam, but was slower and did much worse\n",
    "\n",
    "best_loss = 5 # best validation loss achieved in all models\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    count = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        count += 1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        running_loss += loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = running_loss/count\n",
    "    \n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    count = 0\n",
    "    for inputs, targets in val_dataloader:\n",
    "        count += 1\n",
    "        with torch.no_grad():\n",
    "            running_loss += loss_fn(model(inputs), targets)\n",
    "    val_loss = running_loss / count\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        #torch.save(model.state_dict(), 'best-model-parameters-coordinates.pt')    \n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {train_loss}, Test Loss: {val_loss}')\n",
    "\n",
    "print('Training complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using mean absolute percentage error and MSE to see how well the model did\n",
    "\n",
    "Decent error on training (20%), Best I have gotten so far on validation (50%)\n",
    "\n",
    "Good MSE on validation, around 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.21980761, 0.49818236)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_out = model(train_am).detach().numpy()\n",
    "    val_out = model(valid_am).detach().numpy()\n",
    "mean_absolute_percentage_error(train_label, train_out), mean_absolute_percentage_error(valid_label, val_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.9301880598068237, 5.239727973937988)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(torch.tensor(train_out, dtype=torch.float32), train_label).item(),loss_fn(torch.tensor(val_out, dtype=torch.float32), valid_label).item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
